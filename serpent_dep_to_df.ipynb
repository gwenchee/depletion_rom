{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import scipy.io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mat_matrix(filename, category, normalization=False, filter_light_isos=True):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    read = False\n",
    "    for indx, line in enumerate(lines):\n",
    "        if ';' in line:\n",
    "            read = False\n",
    "        if read:\n",
    "            vector = line.split(' ')\n",
    "            # parse out %\n",
    "            comment_indx = vector.index('%')\n",
    "            vector = vector[:comment_indx]\n",
    "            matrix = np.vstack((matrix, vector))\n",
    "        if category in line:\n",
    "            read = True\n",
    "            vector = lines[indx+1].split(' ')\n",
    "            comment_indx = vector.index('%')\n",
    "            length = len(vector[:comment_indx])\n",
    "            matrix = np.empty(length)\n",
    "    # get rid of first line because it's the empty one\n",
    "    # get rid of the last two because they are lost and total\n",
    "    matrix = np.array(matrix[1:], dtype='float64')[:-2]\n",
    "    \n",
    "    if filter_light_isos:\n",
    "        matrix[:15, :] = np.zeros((15, np.shape(matrix)[1]))\n",
    "    \n",
    "    if normalization:\n",
    "        new_matrix = np.empty(np.shape(matrix))\n",
    "        for i in range(np.shape(matrix)[1]):\n",
    "            new_matrix[:,i] = normalize(matrix[:,i])\n",
    "        return new_matrix\n",
    "    # no lost and total\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def parse_mat_name(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    read = False\n",
    "    names = []\n",
    "    for line in lines:\n",
    "        if ';' in line:\n",
    "            read = False\n",
    "        if read:\n",
    "            iso = line.replace(' ', '').replace(\"'\", '').strip()\n",
    "            names.append(iso)\n",
    "        if 'NAMES' in line:\n",
    "            read = True\n",
    "    # get rid of lost and total\n",
    "    return names[:-2]\n",
    "\n",
    "def parse_mat_burnup(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if 'TOT_BURNUP' in line:\n",
    "            bu = line.split()\n",
    "            a = bu.index('[') + 1\n",
    "            z = bu.index('];')\n",
    "            bu = bu[a:z]\n",
    "    return np.array(bu, dtype='float64') * 1e3\n",
    "\n",
    "def name_and_matrix_to_dict(name_list, matrix):\n",
    "    return_dict = {}\n",
    "    for indx, val in enumerate(name_list):\n",
    "        return_dict[val] = matrix[indx]\n",
    "    return return_dict\n",
    "\n",
    "def normalize(array):\n",
    "    new_array = np.empty(len(array))\n",
    "    for indx,val in enumerate(array):\n",
    "        new_array[indx] = val / sum(array)\n",
    "    return new_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically collect SERPENT dep file into pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3.3': './serpent_test_burnup/case_1_enrichment_3.3%',\n",
       " '4.9': './serpent_test_burnup/case_10_enrichment_4.9%',\n",
       " '4.1': './serpent_test_burnup/case_6_enrichment_4.1%',\n",
       " '3.2': './serpent_test_burnup/case_7_enrichment_3.2%',\n",
       " '4.7': './serpent_test_burnup/case_5_enrichment_4.7%',\n",
       " '4.6': './serpent_test_burnup/case_2_enrichment_4.6%',\n",
       " '3.5': './serpent_test_burnup/case_9_enrichment_3.5%',\n",
       " '3.9': './serpent_test_burnup/case_3_enrichment_3.9%',\n",
       " '3.7': './serpent_test_burnup/case_4_enrichment_3.7%',\n",
       " '3.6': './serpent_test_burnup/case_8_enrichment_3.6%'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do it for all dep files\n",
    "import os\n",
    "subdirectory_list = [x[0] for x in os.walk('./serpent_test_burnup')][1:]\n",
    "dir_dict = {}\n",
    "for subdir in subdirectory_list:\n",
    "    enr = subdir.split('_')[-1]\n",
    "    enr = enr.replace('%', '')\n",
    "    dir_dict[enr] =  subdir\n",
    "dir_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_dict = {}\n",
    "first = True\n",
    "for enr, path in dir_dict.items():\n",
    "    filename = path + '/pwr_dep.m'\n",
    "    name_matrix_dict = name_and_matrix_to_dict(name_list = parse_mat_name(filename),\n",
    "                        matrix = parse_mat_matrix(filename, 'MAT_UO2_MDENS',\n",
    "                                                  normalization=True,\n",
    "                                                  filter_light_isos=True))\n",
    "    name_matrix_dict['bu'] = parse_mat_burnup(filename)\n",
    "    name_matrix_dict['enr'] = [enr] * len(name_matrix_dict['bu'])\n",
    "    coll_dict[enr] = name_matrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NUMBER OF DEPLETION CALCULATIONS SHOULD BE: \n",
      "[enrichment variation * burnup steps]\n",
      "10 * 21 = 210\n"
     ]
    }
   ],
   "source": [
    "enr_num = len(dir_dict.keys())\n",
    "bu_num = len(parse_mat_burnup(filename))\n",
    "print('TOTAL NUMBER OF DEPLETION CALCULATIONS SHOULD BE: \\n'\n",
    "      '[enrichment variation * burnup steps]\\n'\n",
    "      '%i * %i = %i'\n",
    "%(enr_num, bu_num, enr_num * bu_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "first_key = list(coll_dict.keys())[0]\n",
    "for category in coll_dict[first_key].keys():\n",
    "    long_vec = np.empty(0)\n",
    "    for key, val in coll_dict.items():\n",
    "        long_vec = np.append(long_vec, val[category])\n",
    "    df_dict[category] = long_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change column name to fit udb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_hyphen(string):\n",
    "    for indx, i in enumerate(string):\n",
    "        if i.isdigit():\n",
    "            site = indx\n",
    "            break\n",
    "    with_hyphen = string[:site] + '-' + string[site:]\n",
    "    return with_hyphen.lower()\n",
    "\n",
    "def does_it_have_number(string):\n",
    "    for i in string:\n",
    "        if i.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "change_dict = {}\n",
    "for i in list(df):\n",
    "    if does_it_have_number(i):\n",
    "        change_dict[i] = insert_hyphen(i)\n",
    "    else:\n",
    "        change_dict[i] = i\n",
    "\n",
    "change_dict['enr'] = 'init_enr'\n",
    "\n",
    "df =df.rename(index=str, columns = change_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export UDB and SCALE depletion calculations as file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dat = pd.read_csv('./curated.csv', index_col=0)\n",
    "\n",
    "# sift out pwrs\n",
    "all_dat = all_dat.loc[all_dat['reactor_type'] == 'PWR']  \n",
    "all_dat = sklearn.utils.shuffle(all_dat)\n",
    "# only get assemblies with enrichment bigger than 1.5 and bunrup higher than 10,000\n",
    "all_dat = all_dat.loc[(all_dat['init_enr'] > 1.5) & (all_dat['bu'] > 10000)]\n",
    "\n",
    "data_dict = {'UDB': all_dat, 'SERPENT': df}\n",
    "\n",
    "with open('all_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
