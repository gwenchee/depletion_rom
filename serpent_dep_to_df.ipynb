{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import scipy.io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mat_matrix(filename, category, normalization=False):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    read = False\n",
    "    for indx, line in enumerate(lines):\n",
    "        if ';' in line:\n",
    "            read = False\n",
    "        if read:\n",
    "            vector = line.split(' ')\n",
    "            # parse out %\n",
    "            comment_indx = vector.index('%')\n",
    "            vector = vector[:comment_indx]\n",
    "            matrix = np.vstack((matrix, vector))\n",
    "        if category in line:\n",
    "            read = True\n",
    "            vector = lines[indx+1].split(' ')\n",
    "            comment_indx = vector.index('%')\n",
    "            length = len(vector[:comment_indx])\n",
    "            matrix = np.empty(length)\n",
    "    matrix = np.array(matrix[1:], dtype='float64')    \n",
    "    \n",
    "    if normalization:\n",
    "        new_matrix = np.empty(np.shape(matrix))\n",
    "        for i in range(np.shape(matrix)[1]):\n",
    "            new_matrix[:,i] = normalize(matrix[:,i])\n",
    "        return new_matrix[:-2]\n",
    "    # no lost and total\n",
    "    return matrix[:-2]\n",
    "\n",
    "\n",
    "def parse_mat_name(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    read = False\n",
    "    names = []\n",
    "    for line in lines:\n",
    "        if ';' in line:\n",
    "            read = False\n",
    "        if read:\n",
    "            iso = line.replace(' ', '').replace(\"'\", '').strip()\n",
    "            names.append(iso)\n",
    "        if 'NAMES' in line:\n",
    "            read = True\n",
    "    return names[:-2]\n",
    "\n",
    "def parse_mat_burnup(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if 'TOT_BURNUP' in line:\n",
    "            bu = line.split()\n",
    "            a = bu.index('[') + 1\n",
    "            z = bu.index('];')\n",
    "            bu = bu[a:z]\n",
    "    return np.array(bu, dtype='float64') * 1e3\n",
    "\n",
    "def name_and_matrix_to_dict(name_list, matrix):\n",
    "    return_dict = {}\n",
    "    for indx, val in enumerate(name_list):\n",
    "        return_dict[val] = matrix[indx]\n",
    "    return return_dict\n",
    "\n",
    "def normalize(array):\n",
    "    new_array = np.empty(len(array))\n",
    "    for indx,val in enumerate(array):\n",
    "        new_array[indx] = val / sum(array)\n",
    "    return new_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically collect SERPENT dep file into pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3.3': './serpent_test_burnup/case_1_enrichment_3.3%',\n",
       " '4.9': './serpent_test_burnup/case_10_enrichment_4.9%',\n",
       " '4.1': './serpent_test_burnup/case_6_enrichment_4.1%',\n",
       " '3.2': './serpent_test_burnup/case_7_enrichment_3.2%',\n",
       " '4.7': './serpent_test_burnup/case_5_enrichment_4.7%',\n",
       " '4.6': './serpent_test_burnup/case_2_enrichment_4.6%',\n",
       " '3.5': './serpent_test_burnup/case_9_enrichment_3.5%',\n",
       " '3.9': './serpent_test_burnup/case_3_enrichment_3.9%',\n",
       " '3.7': './serpent_test_burnup/case_4_enrichment_3.7%',\n",
       " '3.6': './serpent_test_burnup/case_8_enrichment_3.6%'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do it for all dep files\n",
    "import os\n",
    "subdirectory_list = [x[0] for x in os.walk('./serpent_test_burnup')][1:]\n",
    "dir_dict = {}\n",
    "for subdir in subdirectory_list:\n",
    "    enr = subdir.split('_')[-1]\n",
    "    enr = enr.replace('%', '')\n",
    "    dir_dict[enr] =  subdir\n",
    "dir_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_dict = {}\n",
    "first = True\n",
    "for enr, path in dir_dict.items():\n",
    "    filename = path + '/pwr_dep.m'\n",
    "    name_matrix_dict = name_and_matrix_to_dict(name_list = parse_mat_name(filename),\n",
    "                        matrix = parse_mat_matrix(filename, 'MAT_UO2_MDENS', normalization=True))\n",
    "    name_matrix_dict['bu'] = parse_mat_burnup(filename)\n",
    "    name_matrix_dict['enr'] = [enr] * len(name_matrix_dict['bu'])\n",
    "    coll_dict[enr] = name_matrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NUMBER OF DEPLETION CALCULATIONS SHOULD BE: \n",
      "[enrichment variation * burnup steps]\n",
      "10 * 21 = 210\n"
     ]
    }
   ],
   "source": [
    "enr_num = len(dir_dict.keys())\n",
    "bu_num = len(parse_mat_burnup(filename))\n",
    "print('TOTAL NUMBER OF DEPLETION CALCULATIONS SHOULD BE: \\n'\n",
    "      '[enrichment variation * burnup steps]\\n'\n",
    "      '%i * %i = %i'\n",
    "%(enr_num, bu_num, enr_num * bu_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "first_key = list(coll_dict.keys())[0]\n",
    "for category in coll_dict[first_key].keys():\n",
    "    long_vec = np.empty(0)\n",
    "    for key, val in coll_dict.items():\n",
    "        long_vec = np.append(long_vec, val[category])\n",
    "    df_dict[category] = long_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export UDB and SCALE depletion calculations as file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dat = pd.read_csv('./curated.csv', index_col=0)\n",
    "\n",
    "# sift out pwrs\n",
    "all_dat = all_dat.loc[all_dat['reactor_type'] == 'PWR']  \n",
    "all_dat = sklearn.utils.shuffle(all_dat)\n",
    "# only get assemblies with enrichment bigger than 1.5 and bunrup higher than 10,000\n",
    "all_dat = all_dat.loc[(all_dat['init_enr'] > 1.5) & (all_dat['bu'] > 10000)]\n",
    "\n",
    "data_dict = {'UDB': all_dat, 'SERPENT': df}\n",
    "\n",
    "with open('all_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
